{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "c4YLRPkwgYGr",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    },
    "outputId": "1783b988-bd1a-4cc4-b1d1-fd35ac6bbc9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "423\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will proot for authorization\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import io\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "hm_lines = 10000000\n",
    "\n",
    "def create_lexicon(pos, neg):\n",
    "  lexicon = []\n",
    "  for fi in [pos, neg]:\n",
    "    with io.open(fi, 'r', encoding='cp437') as f:\n",
    "      contents = f.readlines()\n",
    "      for l in contents[:hm_lines]:\n",
    "        all_words = word_tokenize(l.lower())\n",
    "        lexicon += list(all_words)\n",
    "  \n",
    "  lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "  w_counts = Counter(lexicon)\n",
    "  l2 = []\n",
    "  for w in w_counts:\n",
    "    if 1000 > w_counts[w] > 50:\n",
    "      l2.append(w)\n",
    "  print(len(l2))\n",
    "  return l2\n",
    "  \n",
    "def sample_handling(sample, lexicon, classification):\n",
    "  featureset = []\n",
    "  \n",
    "  with io.open(sample, 'r', encoding='cp437') as f:\n",
    "    contents = f.readlines()\n",
    "    for l in contents[:hm_lines]:\n",
    "      current_words = word_tokenize(l.lower())\n",
    "      current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "      features = np.zeros(len(lexicon))\n",
    "      for word in current_words:\n",
    "        if word.lower() in lexicon:\n",
    "          index_value = lexicon.index(word.lower())\n",
    "          features[index_value] += 1\n",
    "      features = list(features)\n",
    "      featureset.append([features, classification])\n",
    "      \n",
    "  return featureset\n",
    "\n",
    "def create_feature_sets_and_labels(pos, neg, test_size=0.1):\n",
    "  lexicon = create_lexicon(pos, neg)\n",
    "  features = []\n",
    "  pos_path = '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/pos.txt'\n",
    "  neg_path = '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/neg.txt'\n",
    "  features += sample_handling(pos_path, lexicon, [1, 0])\n",
    "  features += sample_handling(neg_path, lexicon, [0, 1])\n",
    "  random.shuffle(features)\n",
    "  \n",
    "  '''\n",
    "  does tf.argmax([output]) == tf.argmax([expectations])\n",
    "  tf.argmax([4234, 7923]) == tf.argmax([1, 0])\n",
    "  # neural net will try to shift weights to make statement true\n",
    "  '''\n",
    "  features = np.array(features)\n",
    "  testing_size = int(test_size*len(features))\n",
    "  \n",
    "  train_x = list(features[:,0][:-testing_size])\n",
    "  train_y = list(features[:,1][:-testing_size])\n",
    "  \n",
    "  test_x = list(features[:,0][-testing_size:])\n",
    "  test_y = list(features[:,1][-testing_size:])\n",
    "  \n",
    "  return train_x, train_y, test_x, test_y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  pos_path = '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/pos.txt'\n",
    "  neg_path = '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/neg.txt'\n",
    "  train_x, train_y, test_x, test_y = create_feature_sets_and_labels(pos_path, neg_path)\n",
    "  with open('/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/sentiment_set.pickle', 'wb') as f:\n",
    "    pickle.dump([train_x, train_y, test_x, test_y], f)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "dY4TCaas3RF6",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will proot for authorization\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# After executing the cell above, Drive\n",
    "# files will be present in \"/content/drive/My Drive\".\n",
    "!ls \"/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ProcessingData.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
