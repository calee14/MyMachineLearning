{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProcessingData.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "c4YLRPkwgYGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1783b988-bd1a-4cc4-b1d1-fd35ac6bbc9f"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will proot for authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import io\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "hm_lines = 10000000\n",
        "\n",
        "def create_lexicon(pos, neg):\n",
        "  lexicon = []\n",
        "  for fi in [pos, neg]:\n",
        "    with io.open(fi, 'r', encoding='cp437') as f:\n",
        "      contents = f.readlines()\n",
        "      for l in contents[:hm_lines]:\n",
        "        all_words = word_tokenize(l.lower())\n",
        "        lexicon += list(all_words)\n",
        "  \n",
        "  lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
        "  w_counts = Counter(lexicon)\n",
        "  l2 = []\n",
        "  for w in w_counts:\n",
        "    if 1000 > w_counts[w] > 50:\n",
        "      l2.append(w)\n",
        "  print(len(l2))\n",
        "  return l2\n",
        "  \n",
        "def sample_handling(sample, lexicon, classification):\n",
        "  featureset = []\n",
        "  \n",
        "  with io.open(sample, 'r', encoding='cp437') as f:\n",
        "    contents = f.readlines()\n",
        "    for l in contents[:hm_lines]:\n",
        "      current_words = word_tokenize(l.lower())\n",
        "      current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "      features = np.zeros(len(lexicon))\n",
        "      for word in current_words:\n",
        "        if word.lower() in lexicon:\n",
        "          index_value = lexicon.index(word.lower())\n",
        "          features[index_value] += 1\n",
        "      features = list(features)\n",
        "      featureset.append([features, classification])\n",
        "      \n",
        "  return featureset\n",
        "\n",
        "def create_feature_sets_and_labels(pos, neg, test_size=0.1):\n",
        "  lexicon = create_lexicon(pos, neg)\n",
        "  features = []\n",
        "  pos_path = '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/pos.txt'\n",
        "  neg_path = '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/neg.txt'\n",
        "  features += sample_handling(pos_path, lexicon, [1, 0])\n",
        "  features += sample_handling(neg_path, lexicon, [0, 1])\n",
        "  random.shuffle(features)\n",
        "  \n",
        "  '''\n",
        "  does tf.argmax([output]) == tf.argmax([expectations])\n",
        "  tf.argmax([4234, 7923]) == tf.argmax([1, 0])\n",
        "  # neural net will try to shift weights to make statement true\n",
        "  '''\n",
        "  features = np.array(features)\n",
        "  testing_size = int(test_size*len(features))\n",
        "  \n",
        "  train_x = list(features[:,0][:-testing_size])\n",
        "  train_y = list(features[:,1][:-testing_size])\n",
        "  \n",
        "  test_x = list(features[:,0][-testing_size:])\n",
        "  test_y = list(features[:,1][-testing_size:])\n",
        "  \n",
        "  return train_x, train_y, test_x, test_y\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  pos_path = '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/pos.txt'\n",
        "  neg_path = '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/neg.txt'\n",
        "  train_x, train_y, test_x, test_y = create_feature_sets_and_labels(pos_path, neg_path)\n",
        "  with open('/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/sentiment_set.pickle', 'wb') as f:\n",
        "    pickle.dump([train_x, train_y, test_x, test_y], f)\n",
        "  "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dY4TCaas3RF6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will proot for authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}