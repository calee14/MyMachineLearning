{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crypto_rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Js_kfp_9GLNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "525b9fca-ce73-4720-d099-3f999c208d1b"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will proot for authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEQ_LEN = 60 # how long of a proceeding sequence to collect for RNN\n",
        "FUTURE_PERIOD_PREDICT = 3 # how far into the future are we trying to predict?\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\n",
        "\n",
        "def classify(current, future):\n",
        "  if float(future) > float(current):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def preprocess_df(df):\n",
        "  df = df.drop('future', 1) # don't need this anymore\n",
        "  \n",
        "  for col in df.columns: # go through all of the columns\n",
        "    if col != \"target\": # normalize all ... except for the target itself\n",
        "      df[col] = df[col].pct_change() # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "      df.dropna(inplace=True) # remove the nas created by pct_change\n",
        "      df[col] = preprocessing.scale(df[col].values) # scale between 0 and 1\n",
        "      \n",
        "  df.dropna(inplace=True) # cleanup again... jic. Those nasty NaNs love to creep in.\n",
        "  \n",
        "  sequential_data = [] # this is a list that will CONTAIN the sequences\n",
        "  prev_days = deque(maxlen=SEQ_LEN) # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "  \n",
        "  for i in df.values: # iterate over the values\n",
        "    prev_days.append([n for n in i[:-1]]) # store all but the target\n",
        "    if len(prev_days) == SEQ_LEN: # make sure we have 60 sequences\n",
        "      sequential_data.append([np.array(prev_days), i[-1]]) # append those bad boys\n",
        "      \n",
        "  random.shuffle(sequential_data) # shuffle for good measure\n",
        "  \n",
        "  buys = []\n",
        "  sells = []\n",
        "  \n",
        "  for seq, tartet in sequential_data:\n",
        "    if target == 0:\n",
        "      sells.append([seq, target])\n",
        "    elif target == 1:\n",
        "      buys.append([seq, target])\n",
        "      \n",
        "# take featuresets and combine them into sequences of 60 of these featuresets\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"] # the 4 ratios we want to consider\n",
        "for ratio in ratios: # begin iteration\n",
        "  dataset = f\"/content/drive/My Drive/Colab Notebooks/crypto/crypto_data/{ratio}.csv\" # get full path to the file\n",
        "  \n",
        "  df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"]) # read in specific file\n",
        "  \n",
        "  # renaming volume and close to include the ticker so we can see which close/volume is which\n",
        "  df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "  \n",
        "  df.set_index(\"time\", inplace=True) # set time as index so we can join them on this shared time\n",
        "  df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]] # ignore the other columns besides price and volume\n",
        "  \n",
        "  if len(main_df) == 0: # if the dataframe is empty\n",
        "    main_df = df # then it's just the\n",
        "  else: # otherwise, join this data to the main one\n",
        "    main_df = main_df.join(df)\n",
        "  \n",
        "main_df[\"future\"] = main_df[f\"{RATIO_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
        "\n",
        "main_df[\"target\"] = list(map(classify, main_df[f\"{RATIO_TO_PREDICT}_close\"], main_df[\"future\"]))\n",
        "\n",
        "# print(main_df[[f\"{RATIO_TO_PREDICT}_close\", \"future\", \"target\"]].head(10))\n",
        "\n",
        "# Scale, normalize and put data in sequences\n",
        "\n",
        "times = sorted(main_df.index.values) # get the times\n",
        "last_5pct = times[-int(0.05*len(times))] # get the last 5% of the times\n",
        "# print(last_5pct)\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)] # make the validation data where the index is in the last 5%\n",
        "main_df = main_df[(main_df.index < last_5pct)] # now the main_df is all the data up to the last 5%\n",
        "\n",
        "preprocess_df(main_df)\n",
        "# train_x, train_y = preprocess_df(main_df)\n",
        "# validation_x, validation_y = preprocess_df(validation_main_df)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "1534922100\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}