{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crypto_rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Js_kfp_9GLNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "39ce15d2-74fd-4d6e-9f77-e07472f0a17b"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will proot for authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEQ_LEN = 60 # how long of a proceeding sequence to collect for RNN\n",
        "FUTURE_PERIOD_PREDICT = 3 # how far into the future are we trying to predict?\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\n",
        "\n",
        "def classify(current, future):\n",
        "  if float(future) > float(current):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def preprocess_df(df):\n",
        "  df = df.drop('future', 1) # don't need this anymore\n",
        "  \n",
        "  for col in df.columns: # go through all of the columns\n",
        "    if col != \"target\": # normalize all ... except for the target itself\n",
        "      df[col] = df[col].pct_change() # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "      df.dropna(inplace=True) # remove the nas created by pct_change\n",
        "      df[col] = preprocessing.scale(df[col].values) # scale between 0 and 1\n",
        "      \n",
        "  df.dropna(inplace=True) # cleanup again... jic. Those nasty NaNs love to creep in.\n",
        "  \n",
        "  sequential_data = [] # this is a list that will CONTAIN the sequences\n",
        "  prev_days = deque(maxlen=SEQ_LEN) # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "  \n",
        "  for i in df.values: # iterate over the values\n",
        "    prev_days.append([n for n in i[:-1]]) # store all but the target\n",
        "    if len(prev_days) == SEQ_LEN: # make sure we have 60 sequences\n",
        "      sequential_data.append([np.array(prev_days), i[-1]]) # append those bad boys\n",
        "      \n",
        "  random.shuffle(sequential_data) # shuffle for good measure\n",
        "  \n",
        "  buys = [] # list that will store our buy sequences and targets\n",
        "  sells = [] # list that will store our sell sequences and targets\n",
        "  \n",
        "  for seq, target in sequential_data: # iterate over the sequential data\n",
        "    if target == 0: # if it's a \"not buy\"\n",
        "      sells.append([seq, target]) # append to sells list\n",
        "    elif target == 1: # otherwise if the target is a 1...\n",
        "      buys.append([seq, target]) # it's a buy\n",
        "      \n",
        "  random.shuffle(buys) # shuffle the buys\n",
        "  random.shuffle(sells) # shuffle the sells\n",
        "  \n",
        "  lower = min(len(buys), len(sells)) # what's the shorted length?\n",
        "  \n",
        "  buys = buys[:lower] # make sure both lists are only up to the shortest length\n",
        "  sells = sells[:lower] # make sure both lists are onyl up to the shortest length\n",
        "  \n",
        "  sequential_data = buys+sells # add them together\n",
        "  random.shuffle(sequential_data) # another shuffle, so the model doesn't get confused will all 1 class then the other.\n",
        "  \n",
        "  X = []\n",
        "  y = []\n",
        "  \n",
        "  for seq, target in sequential_data: # going over our new sequential data\n",
        "    X.append(seq) # X is the sequences\n",
        "    y.append(target) # y is the targets/labels (buys vs sell/not buy)\n",
        "  \n",
        "  return np.array(X), y # return X and y...and make X a numpy array!\n",
        "  \n",
        "# take featuresets and combine them into sequences of 60 of these featuresets\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"] # the 4 ratios we want to consider\n",
        "for ratio in ratios: # begin iteration\n",
        "  dataset = f\"/content/drive/My Drive/Colab Notebooks/crypto/crypto_data/{ratio}.csv\" # get full path to the file\n",
        "  \n",
        "  df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"]) # read in specific file\n",
        "  \n",
        "  # renaming volume and close to include the ticker so we can see which close/volume is which\n",
        "  df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "  \n",
        "  df.set_index(\"time\", inplace=True) # set time as index so we can join them on this shared time\n",
        "  df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]] # ignore the other columns besides price and volume\n",
        "  \n",
        "  if len(main_df) == 0: # if the dataframe is empty\n",
        "    main_df = df # then it's just the\n",
        "  else: # otherwise, join this data to the main one\n",
        "    main_df = main_df.join(df)\n",
        "  \n",
        "main_df[\"future\"] = main_df[f\"{RATIO_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
        "\n",
        "main_df[\"target\"] = list(map(classify, main_df[f\"{RATIO_TO_PREDICT}_close\"], main_df[\"future\"]))\n",
        "\n",
        "# print(main_df[[f\"{RATIO_TO_PREDICT}_close\", \"future\", \"target\"]].head(10))\n",
        "\n",
        "# Scale, normalize and put data in sequences\n",
        "\n",
        "times = sorted(main_df.index.values) # get the times\n",
        "last_5pct = times[-int(0.05*len(times))] # get the last 5% of the times\n",
        "# print(last_5pct)\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)] # make the validation data where the index is in the last 5%\n",
        "main_df = main_df[(main_df.index < last_5pct)] # now the main_df is all the data up to the last 5%\n",
        "\n",
        "# preprocess_df(main_df)\n",
        "# preprocess the data\n",
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "\n",
        "# print some stats\n",
        "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "train data: 69188 validation: 3062\n",
            "Dont buys: 34594, buys: 34594\n",
            "VALIDATION Dont buys: 1531, buys: 1531\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}