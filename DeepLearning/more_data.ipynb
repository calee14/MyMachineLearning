{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "more_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "EJpeoYlVG-88",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "'''\n",
        "polarity 0 = negative. 2 = neutral. 4 = positive.\n",
        "id\n",
        "date\n",
        "query\n",
        "user\n",
        "tweet\n",
        "'''\n",
        "\n",
        "def init_process(fin,fout):\n",
        "\toutfile = open(fout,'a')\n",
        "\twith open(fin, buffering=200000, encoding='latin-1') as f:\n",
        "\t\ttry:\n",
        "\t\t\tfor line in f:\n",
        "\t\t\t\tline = line.replace('\"','')\n",
        "\t\t\t\tinitial_polarity = line.split(',')[0]\n",
        "\t\t\t\tif initial_polarity == '0':\n",
        "\t\t\t\t\tinitial_polarity = [1,0]\n",
        "\t\t\t\telif initial_polarity == '4':\n",
        "\t\t\t\t\tinitial_polarity = [0,1]\n",
        "\n",
        "\t\t\t\ttweet = line.split(',')[-1]\n",
        "\t\t\t\toutline = str(initial_polarity)+':::'+tweet\n",
        "\t\t\t\toutfile.write(outline)\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tprint(str(e))\n",
        "\toutfile.close()\n",
        "\n",
        "init_process('training.1600000.processed.noemoticon.csv','train_set.csv')\n",
        "init_process('testdata.manual.2009.06.14.csv','test_set.csv')\n",
        "\n",
        "\n",
        "def create_lexicon(fin):\n",
        "\tlexicon = []\n",
        "\twith open(fin, 'r', buffering=100000, encoding='latin-1') as f:\n",
        "\t\ttry:\n",
        "\t\t\tcounter = 1\n",
        "\t\t\tcontent = ''\n",
        "\t\t\tfor line in f:\n",
        "\t\t\t\tcounter += 1\n",
        "\t\t\t\tif (counter/2500.0).is_integer():\n",
        "\t\t\t\t\ttweet = line.split(':::')[1]\n",
        "\t\t\t\t\tcontent += ' '+tweet\n",
        "\t\t\t\t\twords = word_tokenize(content)\n",
        "\t\t\t\t\twords = [lemmatizer.lemmatize(i) for i in words]\n",
        "\t\t\t\t\tlexicon = list(set(lexicon + words))\n",
        "\t\t\t\t\tprint(counter, len(lexicon))\n",
        "\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tprint(str(e))\n",
        "\n",
        "\twith open('lexicon-2500-2638.pickle','wb') as f:\n",
        "\t\tpickle.dump(lexicon,f)\n",
        "\n",
        "create_lexicon('train_set.csv')\n",
        "\n",
        "\n",
        "def convert_to_vec(fin,fout,lexicon_pickle):\n",
        "\twith open(lexicon_pickle,'rb') as f:\n",
        "\t\tlexicon = pickle.load(f)\n",
        "\toutfile = open(fout,'a')\n",
        "\twith open(fin, buffering=20000, encoding='latin-1') as f:\n",
        "\t\tcounter = 0\n",
        "\t\tfor line in f:\n",
        "\t\t\tcounter +=1\n",
        "\t\t\tlabel = line.split(':::')[0]\n",
        "\t\t\ttweet = line.split(':::')[1]\n",
        "\t\t\tcurrent_words = word_tokenize(tweet.lower())\n",
        "\t\t\tcurrent_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "\n",
        "\t\t\tfeatures = np.zeros(len(lexicon))\n",
        "\n",
        "\t\t\tfor word in current_words:\n",
        "\t\t\t\tif word.lower() in lexicon:\n",
        "\t\t\t\t\tindex_value = lexicon.index(word.lower())\n",
        "\t\t\t\t\t# OR DO +=1, test both\n",
        "\t\t\t\t\tfeatures[index_value] += 1\n",
        "\n",
        "\t\t\tfeatures = list(features)\n",
        "\t\t\toutline = str(features)+'::'+str(label)+'\\n'\n",
        "\t\t\toutfile.write(outline)\n",
        "\n",
        "\t\tprint(counter)\n",
        "\n",
        "convert_to_vec('test_set.csv','processed-test-set.csv','lexicon-2500-2638.pickle')\n",
        "\n",
        "\n",
        "def shuffle_data(fin):\n",
        "\tdf = pd.read_csv(fin, error_bad_lines=False)\n",
        "\tdf = df.iloc[np.random.permutation(len(df))]\n",
        "\tprint(df.head())\n",
        "\tdf.to_csv('train_set_shuffled.csv', index=False)\n",
        "\t\n",
        "shuffle_data('train_set.csv')\n",
        "\n",
        "\n",
        "def create_test_data_pickle(fin):\n",
        "\n",
        "\tfeature_sets = []\n",
        "\tlabels = []\n",
        "\tcounter = 0\n",
        "\twith open(fin, buffering=20000) as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tfeatures = list(eval(line.split('::')[0]))\n",
        "\t\t\t\tlabel = list(eval(line.split('::')[1]))\n",
        "\n",
        "\t\t\t\tfeature_sets.append(features)\n",
        "\t\t\t\tlabels.append(label)\n",
        "\t\t\t\tcounter += 1\n",
        "\t\t\texcept:\n",
        "\t\t\t\tpass\n",
        "\tprint(counter)\n",
        "\tfeature_sets = np.array(feature_sets)\n",
        "\tlabels = np.array(labels)\n",
        "\n",
        "create_test_data_pickle('processed-test-set.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bhDmH2i5MLN5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "n_nodes_hl1 = 500\n",
        "n_nodes_hl2 = 500\n",
        "\n",
        "n_classes = 2\n",
        "\n",
        "batch_size = 32\n",
        "total_batches = int(1600000/batch_size)\n",
        "hm_epochs = 10\n",
        "\n",
        "x = tf.placeholder('float')\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
        "                  'weight':tf.Variable(tf.random_normal([2638, n_nodes_hl1])),\n",
        "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
        "\n",
        "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
        "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
        "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
        "\n",
        "output_layer = {'f_fum':None,\n",
        "                'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_classes])),\n",
        "                'bias':tf.Variable(tf.random_normal([n_classes])),}\n",
        "\n",
        "def neural_network_model(data):\n",
        "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
        "    l1 = tf.nn.relu(l1)\n",
        "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
        "    l2 = tf.nn.relu(l2)\n",
        "    output = tf.matmul(l2,output_layer['weight']) + output_layer['bias']\n",
        "    return output\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "tf_log = 'tf.log'\n",
        "\n",
        "def train_neural_network(x):\n",
        "    prediction = neural_network_model(x)\n",
        "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        try:\n",
        "            epoch = int(open(tf_log,'r').read().split('\\n')[-2])+1\n",
        "            print('STARTING:',epoch)\n",
        "        except:\n",
        "            epoch = 1\n",
        "\n",
        "        while epoch <= hm_epochs:\n",
        "            if epoch != 1:\n",
        "                saver.restore(sess,'/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/' + \"model.ckpt\")\n",
        "            epoch_loss = 1\n",
        "            with open('/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/' + 'lexicon.pickle','rb') as f:\n",
        "                lexicon = pickle.load(f)\n",
        "            with open('/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/' + 'train_set_shuffled.csv', buffering=20000, encoding='latin-1') as f:\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "                batches_run = 0\n",
        "                for line in f:\n",
        "                    label = line.split(':::')[0]\n",
        "                    tweet = line.split(':::')[1]\n",
        "                    current_words = word_tokenize(tweet.lower())\n",
        "                    current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "\n",
        "                    features = np.zeros(len(lexicon))\n",
        "\n",
        "                    for word in current_words:\n",
        "                        if word.lower() in lexicon:\n",
        "                            index_value = lexicon.index(word.lower())\n",
        "                            # OR DO +=1, test both\n",
        "                            features[index_value] += 1\n",
        "                    line_x = list(features)\n",
        "                    line_y = eval(label)\n",
        "                    batch_x.append(line_x)\n",
        "                    batch_y.append(line_y)\n",
        "                    if len(batch_x) >= batch_size:\n",
        "                        _, c = sess.run([optimizer, cost], feed_dict={x: np.array(batch_x),\n",
        "                                                                  y: np.array(batch_y)})\n",
        "                        epoch_loss += c\n",
        "                        batch_x = []\n",
        "                        batch_y = []\n",
        "                        batches_run +=1\n",
        "                        print('Batch run:',batches_run,'/',total_batches,'| Epoch:',epoch,'| Batch Loss:',c,)\n",
        "\n",
        "            saver.save(sess, '/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/' + \"model.ckpt\")\n",
        "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "            with open(tf_log,'a') as f:\n",
        "                f.write(str(epoch)+'\\n') \n",
        "            epoch +=1\n",
        "\n",
        "train_neural_network(x)\n",
        "\n",
        "def test_neural_network():\n",
        "    prediction = neural_network_model(x)\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        for epoch in range(hm_epochs):\n",
        "            try:\n",
        "                saver.restore(sess,'/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/' + \"model.ckpt\")\n",
        "            except Exception as e:\n",
        "                print(str(e))\n",
        "            epoch_loss = 0\n",
        "            \n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "        feature_sets = []\n",
        "        labels = []\n",
        "        counter = 0\n",
        "        with open('/content/drive/My Drive/Colab Notebooks/ProcessingOurOwnData/' + 'processed-test-set.csv', buffering=20000) as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    features = list(eval(line.split('::')[0]))\n",
        "                    label = list(eval(line.split('::')[1]))\n",
        "                    feature_sets.append(features)\n",
        "                    labels.append(label)\n",
        "                    counter += 1\n",
        "                except:\n",
        "                    pass\n",
        "        print('Tested',counter,'samples.')\n",
        "        test_x = np.array(feature_sets)\n",
        "        test_y = np.array(labels)\n",
        "        print('Accuracy:',accuracy.eval({x:test_x, y:test_y}))\n",
        "\n",
        "\n",
        "test_neural_network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4fNeWGeGWZEB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "n_nodes_hl1 = 500\n",
        "n_nodes_hl2 = 500\n",
        "\n",
        "n_classes = 2\n",
        "hm_data = 2000000\n",
        "\n",
        "batch_size = 32\n",
        "hm_epochs = 10\n",
        "\n",
        "x = tf.placeholder('float')\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "\n",
        "current_epoch = tf.Variable(1)\n",
        "\n",
        "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
        "                  'weight':tf.Variable(tf.random_normal([2638, n_nodes_hl1])),\n",
        "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
        "\n",
        "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
        "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
        "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
        "\n",
        "output_layer = {'f_fum':None,\n",
        "                'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_classes])),\n",
        "                'bias':tf.Variable(tf.random_normal([n_classes])),}\n",
        "\n",
        "\n",
        "def neural_network_model(data):\n",
        "\n",
        "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
        "    l1 = tf.nn.relu(l1)\n",
        "\n",
        "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
        "    l2 = tf.nn.relu(l2)\n",
        "\n",
        "    output = tf.matmul(l2,output_layer['weight']) + output_layer['bias']\n",
        "\n",
        "    return output\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "def use_neural_network(input_data):\n",
        "    prediction = neural_network_model(x)\n",
        "    with open('lexicon.pickle','rb') as f:\n",
        "        lexicon = pickle.load(f)\n",
        "        \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        saver.restore(sess,\"model.ckpt\")\n",
        "        current_words = word_tokenize(input_data.lower())\n",
        "        current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "        features = np.zeros(len(lexicon))\n",
        "\n",
        "        for word in current_words:\n",
        "            if word.lower() in lexicon:\n",
        "                index_value = lexicon.index(word.lower())\n",
        "                # OR DO +=1, test both\n",
        "                features[index_value] += 1\n",
        "\n",
        "        features = np.array(list(features))\n",
        "        # pos: [1,0] , argmax: 0\n",
        "        # neg: [0,1] , argmax: 1\n",
        "        result = (sess.run(tf.argmax(prediction.eval(feed_dict={x:[features]}),1)))\n",
        "        if result[0] == 0:\n",
        "            print('Positive:',input_data)\n",
        "        elif result[0] == 1:\n",
        "            print('Negative:',input_data)\n",
        "\n",
        "use_neural_network(\"He's an idiot and a jerk.\")\n",
        "use_neural_network(\"This was the best store i've ever seen.\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}